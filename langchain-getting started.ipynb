{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c943bc4-4abd-4b4b-9a26-8108860c4df2",
   "metadata": {},
   "source": [
    "## Imports for huggingface tokens\n",
    "\n",
    "To generate tokens log in to HugginFace and setup your huggingface [tokens here](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7b902-8dde-46bf-9a52-2cad613493b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3267be-b924-4e3a-ad81-3f03531b5287",
   "metadata": {},
   "source": [
    "## Langchain Basics\n",
    "\n",
    "Langchain is a framework which allows you to build pipelines using Large Language Model. Langchain offers support with HugginFace to load and use any openly available model on HuggingFace. The framework can be used to do summarization, Question Answering, build ChatBots, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e6967f-87ad-413b-91a4-2ed84866d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "# PromptTemplate -> used to generate prompt for the Language model\n",
    "# HuggingFaceHub -> calling the huggingface hub to load LLM for inference\n",
    "# LLMChain -> chain of steps to be done by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e93f2ab-2663-49b1-875b-1c657eb562c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the model page link before, you might have to agree to the terms before being able to call the HuggingFace API\n",
    "\n",
    "t5 = HuggingFaceHub(\n",
    "    repo_id='google/flan-t5-large',\n",
    "    model_kwargs={\n",
    "        \"temperature\":100, \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ceefd798-eb33-4e37-a269-b5b22fea2b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-large', timeout=None)>, repo_id='google/flan-t5-large', task='text2text-generation', model_kwargs={'temperature': 100})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e29e6bc-4700-4619-8a11-3a90a2c335db",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Please answer the following question \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llmchain = LLMChain(prompt=prompt, llm=t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8a1e8cb-8e98-4ffe-a61c-19e1cc910442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the capital of UK\"\n",
    "llmchain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9a7dde3b-84fe-4606-9646-fbd124d77dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 feet'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Convert 5 feet to inches?\"\n",
    "llmchain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5cce1-1d23-44e5-9de7-05301f833de4",
   "metadata": {},
   "source": [
    "### Ask multiple questions\n",
    "Inefficient to invoke the llm for asking multiple questions, can rather club multiple questions in a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "924260eb-94c5-4704-9a19-5a6d19fe69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\n",
    "    {\"question\": \"What is the capital of UK\"},\n",
    "    {\"question\": \"What is strongest currency in the world?\"},\n",
    "    {\"question\": \"Who is the most famous footballer?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc5aa83f-2b9d-412e-80f1-2170a699d51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='london')], [Generation(text='pound')], [Generation(text='gerry taylor')]], llm_output=None, run=[RunInfo(run_id=UUID('314249a4-f428-4f94-aa8f-be1f1af51926')), RunInfo(run_id=UUID('46a68549-bea7-419b-92b7-fae6898faa2f')), RunInfo(run_id=UUID('8e4e407b-2550-4aa6-a175-a17bf13cceba'))])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmchain.generate(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e184fe-2c15-4c4c-9ae7-d4da1e301173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18677403-a108-4918-8972-6c3cefb3914e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Prompting with Langchain\n",
    "Langchain offers an object class for PromptTemplate which helps tune and tweak prompts to be sent to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee60d6-b2a8-4ae1-846e-8cf42953e81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
