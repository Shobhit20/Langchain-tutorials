{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206aa9a3",
   "metadata": {},
   "source": [
    "## LLM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5891951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b75c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bee24f",
   "metadata": {},
   "source": [
    "## Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002f949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature, max_completion_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e41eab",
   "metadata": {},
   "source": [
    "## Open Source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e503c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "  os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter API key for HuggingFace: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492418fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0304d0c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/langchain_huggingface/chat_models/huggingface.py:574\u001b[39m, in \u001b[36mChatHuggingFace._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    568\u001b[39m     params = {\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    570\u001b[39m         **params,\n\u001b[32m    571\u001b[39m         **({\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[32m    572\u001b[39m         **kwargs,\n\u001b[32m    573\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(answer)\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:887\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    884\u001b[39m payload_model = model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m    886\u001b[39m \u001b[38;5;66;03m# Get the provider helper\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m provider_helper = \u001b[43mget_provider_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversational\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_id_or_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpayload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# Prepare the payload\u001b[39;00m\n\u001b[32m    896\u001b[39m parameters = {\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    898\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    915\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    916\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/venvs/langchain/lib/python3.11/site-packages/huggingface_hub/inference/_providers/__init__.py:191\u001b[39m, in \u001b[36mget_provider_helper\u001b[39m\u001b[34m(provider, task, model)\u001b[39m\n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpecifying a model is required when provider is \u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m     provider_mapping = _fetch_inference_provider_mapping(model)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     provider = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprovider_mapping\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.provider\n\u001b[32m    193\u001b[39m provider_tasks = PROVIDERS.get(provider)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_tasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mStopIteration\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6906f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fa1ba9",
   "metadata": {},
   "source": [
    "## Local Load HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cd69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0feb9d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|user|>\\nWhat is the capital of France?</s>\\n<|assistant|>\\nThe capital of France is Paris.', additional_kwargs={}, response_metadata={}, id='run--f2aa59d2-6af1-438f-bf0b-a457fb4c26ea-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0.7, \"max_new_tokens\": 100},\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "model.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d34491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e831ecb2",
   "metadata": {},
   "source": [
    "## Embedding models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ee9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\", dimensions=1024)\n",
    "text = \"The capital of France is Paris.\"\n",
    "embedding = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c709a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding)  # Length of the embedding vector\n",
    "\n",
    "embed_document = embeddings.embed_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99147088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3b1c7b",
   "metadata": {},
   "source": [
    "## Embedding Open Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d311e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shobhitmaheshwari/Documents/venvs/langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",)\n",
    "\n",
    "text = \"The capital of France is Paris.\"\n",
    "embedding = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e19d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1032569408416748,\n",
       " 0.030420120805501938,\n",
       " 0.02909577265381813,\n",
       " -0.037322863936424255,\n",
       " 0.078676238656044,\n",
       " -0.05472308024764061,\n",
       " 0.0026922838296741247,\n",
       " -0.008628075942397118,\n",
       " 0.023342935368418694,\n",
       " 0.024929197505116463,\n",
       " -0.0176668893545866,\n",
       " -0.07516758143901825,\n",
       " 0.024401631206274033,\n",
       " -0.05606142058968544,\n",
       " -0.035808682441711426,\n",
       " -0.12067259103059769,\n",
       " -0.0008131168433465064,\n",
       " -0.025866059586405754,\n",
       " 0.04106556624174118,\n",
       " 0.006101945880800486,\n",
       " 0.007386340759694576,\n",
       " -0.035924915224313736,\n",
       " 0.08720473945140839,\n",
       " -0.0009175539598800242,\n",
       " -0.043363429605960846,\n",
       " -0.01564173772931099,\n",
       " -0.05735722556710243,\n",
       " -0.009901881217956543,\n",
       " -0.011077442206442356,\n",
       " 0.002180590759962797,\n",
       " 0.06393151730298996,\n",
       " -0.02309073507785797,\n",
       " -0.06355898082256317,\n",
       " -0.0022083204239606857,\n",
       " -0.03933548927307129,\n",
       " -0.0232898760586977,\n",
       " -0.0037738983519375324,\n",
       " -0.02739354781806469,\n",
       " 0.0436946377158165,\n",
       " 0.04251300171017647,\n",
       " 0.006501066964119673,\n",
       " -0.04662175104022026,\n",
       " -0.05291168764233589,\n",
       " -0.02417188324034214,\n",
       " -0.01890845037996769,\n",
       " 0.022131819278001785,\n",
       " 0.02040284126996994,\n",
       " 0.04285925254225731,\n",
       " 0.026099031791090965,\n",
       " -0.08849188685417175,\n",
       " 0.07116281986236572,\n",
       " 0.024021398276090622,\n",
       " -0.03532208502292633,\n",
       " -0.043641407042741776,\n",
       " -0.06096644327044487,\n",
       " 0.11283935606479645,\n",
       " 0.03445963189005852,\n",
       " 0.01612331159412861,\n",
       " -0.0013744195457547903,\n",
       " 0.0858803316950798,\n",
       " -0.001840704120695591,\n",
       " -0.035088516771793365,\n",
       " -0.0253217164427042,\n",
       " 0.04750654101371765,\n",
       " 0.0866730734705925,\n",
       " 0.020109878852963448,\n",
       " 0.013610883615911007,\n",
       " 0.02650999091565609,\n",
       " -0.10106431692838669,\n",
       " 0.04438764229416847,\n",
       " -0.022757550701498985,\n",
       " -0.003287826431915164,\n",
       " 0.002809001598507166,\n",
       " -0.050846658647060394,\n",
       " 0.10168963670730591,\n",
       " -0.035898804664611816,\n",
       " -0.09472144395112991,\n",
       " 0.0643034428358078,\n",
       " 0.012864342890679836,\n",
       " 0.01661437191069126,\n",
       " 0.049080319702625275,\n",
       " -0.07124507427215576,\n",
       " 0.03968914598226547,\n",
       " 0.02093147486448288,\n",
       " 0.03209412097930908,\n",
       " 0.0035253961104899645,\n",
       " 0.07956987619400024,\n",
       " 0.028938287869095802,\n",
       " 0.05784083530306816,\n",
       " -0.041605256497859955,\n",
       " -0.06631410866975784,\n",
       " 0.019567109644412994,\n",
       " -0.03223385661840439,\n",
       " 0.07654819637537003,\n",
       " -0.11404390633106232,\n",
       " 0.04937826842069626,\n",
       " -0.03456909582018852,\n",
       " 0.052705682814121246,\n",
       " -0.06090128421783447,\n",
       " 0.01627182960510254,\n",
       " -0.03995494917035103,\n",
       " 0.007748160045593977,\n",
       " 0.087015300989151,\n",
       " -0.04845119267702103,\n",
       " -0.014171590097248554,\n",
       " 0.04000863432884216,\n",
       " -0.010629170574247837,\n",
       " -0.023935051634907722,\n",
       " -0.026437195017933846,\n",
       " -0.06552691757678986,\n",
       " -0.08854871988296509,\n",
       " -0.03491685166954994,\n",
       " 0.04319386929273605,\n",
       " -0.025911269709467888,\n",
       " 0.0247916579246521,\n",
       " 0.07188785821199417,\n",
       " -0.029676569625735283,\n",
       " -0.10663484781980515,\n",
       " -0.004752954933792353,\n",
       " -0.05615256726741791,\n",
       " -0.03400403633713722,\n",
       " -0.044908586889505386,\n",
       " -0.021860593929886818,\n",
       " 0.020721450448036194,\n",
       " -0.03847246244549751,\n",
       " -0.005179704166948795,\n",
       " -0.029843652620911598,\n",
       " -8.184746759584809e-33,\n",
       " -0.023101873695850372,\n",
       " 0.09018468111753464,\n",
       " 0.07150405645370483,\n",
       " 0.01443477999418974,\n",
       " -0.02540936879813671,\n",
       " -0.008147201500833035,\n",
       " -0.06493990868330002,\n",
       " 0.05981448292732239,\n",
       " 0.001842841156758368,\n",
       " -0.018718017265200615,\n",
       " 0.03458990156650543,\n",
       " -0.14118130505084991,\n",
       " -0.07314858585596085,\n",
       " 0.06493289023637772,\n",
       " 0.06625650078058243,\n",
       " -0.024059070274233818,\n",
       " 0.14276123046875,\n",
       " 0.06964343041181564,\n",
       " -0.03150387480854988,\n",
       " 0.009132171981036663,\n",
       " 0.04641798511147499,\n",
       " -0.04747268557548523,\n",
       " 0.00913192331790924,\n",
       " 0.026510074734687805,\n",
       " 0.06501849740743637,\n",
       " 0.04513230919837952,\n",
       " -0.01648666150867939,\n",
       " 0.10927714407444,\n",
       " -0.01265273429453373,\n",
       " -0.020349126309156418,\n",
       " -0.045481134206056595,\n",
       " 0.03216315433382988,\n",
       " 0.0612589567899704,\n",
       " 0.012380460277199745,\n",
       " 0.027345454320311546,\n",
       " 0.029306786134839058,\n",
       " 0.02960539422929287,\n",
       " 0.012325813062489033,\n",
       " 0.039619602262973785,\n",
       " 0.0888865515589714,\n",
       " 0.06476441025733948,\n",
       " -0.024953264743089676,\n",
       " -0.025385305285453796,\n",
       " 0.0007495447644032538,\n",
       " 0.0023798756301403046,\n",
       " 0.03593231737613678,\n",
       " -0.019542988389730453,\n",
       " -0.05538873374462128,\n",
       " 0.11693835258483887,\n",
       " 0.0061689442954957485,\n",
       " -0.003917201887816191,\n",
       " -0.056657955050468445,\n",
       " -0.05819907411932945,\n",
       " -0.007994383573532104,\n",
       " 0.04369211569428444,\n",
       " 0.10270003229379654,\n",
       " -0.1082070991396904,\n",
       " -0.002798251574859023,\n",
       " 0.009312774054706097,\n",
       " -0.013998720794916153,\n",
       " -0.006426946260035038,\n",
       " -8.916461956687272e-05,\n",
       " -0.047801997512578964,\n",
       " 0.08564894646406174,\n",
       " 0.0345945730805397,\n",
       " 0.09262138605117798,\n",
       " 0.03017265908420086,\n",
       " 0.027229931205511093,\n",
       " -0.006047939416021109,\n",
       " -0.0497589148581028,\n",
       " -0.06720481067895889,\n",
       " 0.022525453940033913,\n",
       " 0.07965368777513504,\n",
       " 0.05538429319858551,\n",
       " 0.06903732568025589,\n",
       " 0.08348175883293152,\n",
       " -0.030247488990426064,\n",
       " 0.01127556525170803,\n",
       " -0.022626684978604317,\n",
       " 0.001993834273889661,\n",
       " 0.00535175995901227,\n",
       " -0.036800485104322433,\n",
       " -0.080508753657341,\n",
       " 0.01157770212739706,\n",
       " -0.019796915352344513,\n",
       " -0.04278046265244484,\n",
       " 0.012678599916398525,\n",
       " 0.0019794206600636244,\n",
       " -0.005939437076449394,\n",
       " -0.05053455010056496,\n",
       " -0.08363239467144012,\n",
       " -0.12611591815948486,\n",
       " -0.025406628847122192,\n",
       " -0.03182780742645264,\n",
       " -0.0854702964425087,\n",
       " 3.8875177752226965e-33,\n",
       " 0.03479534015059471,\n",
       " 0.03523151949048042,\n",
       " -0.07460110634565353,\n",
       " -0.0005896349321119487,\n",
       " 0.024658482521772385,\n",
       " 0.061496492475271225,\n",
       " 0.07889880239963531,\n",
       " -0.03369031473994255,\n",
       " -0.017659569159150124,\n",
       " 0.017479626461863518,\n",
       " -0.0890812799334526,\n",
       " -0.13845154643058777,\n",
       " 0.013373346999287605,\n",
       " -0.03215283155441284,\n",
       " 0.019738350063562393,\n",
       " 0.05509553849697113,\n",
       " 0.014190085232257843,\n",
       " -0.07091271132230759,\n",
       " -0.03283488377928734,\n",
       " 0.0010392258409410715,\n",
       " -0.08377428352832794,\n",
       " -0.003762910608202219,\n",
       " -0.05125895142555237,\n",
       " 0.021449171006679535,\n",
       " -0.029997946694493294,\n",
       " 0.0011723039206117392,\n",
       " -0.05652701109647751,\n",
       " -0.03225142881274223,\n",
       " 0.0023134355433285236,\n",
       " -0.019587021321058273,\n",
       " -0.07348077744245529,\n",
       " 0.028696931898593903,\n",
       " 0.05434456840157509,\n",
       " 0.11723098158836365,\n",
       " 0.016661180183291435,\n",
       " 0.07929389923810959,\n",
       " 0.030046552419662476,\n",
       " -0.011940978467464447,\n",
       " 0.03317657858133316,\n",
       " 0.02239113673567772,\n",
       " -0.059854306280612946,\n",
       " -0.028867216780781746,\n",
       " 0.029346419498324394,\n",
       " 0.02512459084391594,\n",
       " 0.06510085612535477,\n",
       " 0.02372870221734047,\n",
       " 0.015864474698901176,\n",
       " -0.03657863289117813,\n",
       " -0.00158122053835541,\n",
       " 0.0665377825498581,\n",
       " 0.0337756983935833,\n",
       " 0.040736544877290726,\n",
       " -0.05155531316995621,\n",
       " -0.039431750774383545,\n",
       " -0.006917886435985565,\n",
       " 0.07442133873701096,\n",
       " -0.024408241733908653,\n",
       " 0.03149929642677307,\n",
       " -0.07847961038351059,\n",
       " -0.05354201793670654,\n",
       " 0.03835203871130943,\n",
       " 0.045526716858148575,\n",
       " -0.03220318630337715,\n",
       " 0.07261577248573303,\n",
       " -0.018992578610777855,\n",
       " -0.08240577578544617,\n",
       " -0.09563910216093063,\n",
       " 0.04759342595934868,\n",
       " 0.009751985780894756,\n",
       " 0.027188464999198914,\n",
       " 0.09557440876960754,\n",
       " 0.008518301881849766,\n",
       " -0.061255455017089844,\n",
       " 0.05544334650039673,\n",
       " -0.0491521842777729,\n",
       " 0.006172730587422848,\n",
       " 0.050973374396562576,\n",
       " 0.021447420120239258,\n",
       " -0.006394937634468079,\n",
       " 0.02275446616113186,\n",
       " -0.033357053995132446,\n",
       " 0.022454313933849335,\n",
       " -0.025033630430698395,\n",
       " -0.005829958710819483,\n",
       " -0.04390989616513252,\n",
       " 0.052288200706243515,\n",
       " -0.04268110543489456,\n",
       " -0.08458992093801498,\n",
       " 0.0618283711373806,\n",
       " -0.10084904730319977,\n",
       " -0.003650359343737364,\n",
       " -0.024820635095238686,\n",
       " -0.056905921548604965,\n",
       " -0.032875705510377884,\n",
       " 0.03717947378754616,\n",
       " -1.854076536744742e-08,\n",
       " 0.07703359425067902,\n",
       " 0.02946017123758793,\n",
       " -0.022367754951119423,\n",
       " 0.02281826175749302,\n",
       " -0.05845416337251663,\n",
       " -0.11370285600423813,\n",
       " 0.06529117375612259,\n",
       " -0.000926018925383687,\n",
       " -0.010337555781006813,\n",
       " -0.02542545273900032,\n",
       " -0.10113929212093353,\n",
       " 0.06052374839782715,\n",
       " -0.05062709003686905,\n",
       " -0.06809534132480621,\n",
       " -0.05889594927430153,\n",
       " -0.012464555911719799,\n",
       " -0.02698075771331787,\n",
       " 0.006515855435281992,\n",
       " 0.01625543087720871,\n",
       " 0.06331177800893784,\n",
       " 0.007222969084978104,\n",
       " 0.02399442531168461,\n",
       " -0.036141205579042435,\n",
       " -0.05811728537082672,\n",
       " 0.04445450380444527,\n",
       " -0.10385391861200333,\n",
       " 0.005187711678445339,\n",
       " 0.023536913096904755,\n",
       " -0.046412769705057144,\n",
       " 0.025399422273039818,\n",
       " 0.05735919997096062,\n",
       " -0.06108250841498375,\n",
       " -0.03506498411297798,\n",
       " -0.07840220630168915,\n",
       " 0.034994058310985565,\n",
       " 0.018648412078619003,\n",
       " 0.03658534958958626,\n",
       " 0.01903599128127098,\n",
       " 0.0040017967112362385,\n",
       " -0.00039542492595501244,\n",
       " 0.09371840208768845,\n",
       " 0.01695430651307106,\n",
       " 0.015762820839881897,\n",
       " -0.02584349922835827,\n",
       " 0.026277055963873863,\n",
       " 0.03141576051712036,\n",
       " 0.09418127685785294,\n",
       " -0.06972957402467728,\n",
       " 0.1329268217086792,\n",
       " -0.031633466482162476,\n",
       " -0.0907735824584961,\n",
       " 0.06982333213090897,\n",
       " -0.0227609034627676,\n",
       " -0.012576320208609104,\n",
       " -0.026201259344816208,\n",
       " 0.050604429095983505,\n",
       " -0.10580068826675415,\n",
       " 0.010424097999930382,\n",
       " 0.03285572677850723,\n",
       " -0.03157076612114906,\n",
       " -0.02283315174281597,\n",
       " 0.05853156000375748,\n",
       " 0.08586001396179199,\n",
       " -0.005669801961630583]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53e044",
   "metadata": {},
   "source": [
    "## USE case for embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88da8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\", dimensions=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The capital of Germany is Berlin.\",\n",
    "    \"The capital of Italy is Rome.\",\n",
    "    \"The capital of Spain is Madrid.\",\n",
    "]\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "document_embeddings = embeddings.embed_documents(document_list)\n",
    "similarities = cosine_similarity([query_embedding], document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04df7e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The capital of France is Paris.', np.float64(0.7892893360164778))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index, score = sorted(list(enumerate(similarities)), key=lambda x: x[1], reverse=True)[0]\n",
    "document_list[index], score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13583bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hence the usage of vector db because it allows us to find the most relevant document based on the similarity of their embeddings, which is particularly useful for tasks like semantic search and information retrieval.\n",
    "    without vector db, we would have to compare the query with each document one by one, which is inefficient and not scalable for large datasets and also regenerate the embeddings for each query and documents, which is computationally expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
